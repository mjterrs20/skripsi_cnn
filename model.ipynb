{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import keras.utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import time\n",
    "from keras import metrics\n",
    "\n",
    "MAX_SEQUENCES_LENGTH = 50 # Maximum kata pada kalimat\n",
    "MAX_NB_WORDS = 20000 # Vocabulary size\n",
    "EMBEDDING_DIM = 64 # Dimensions of Glove word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# load data using Python JSON module\n",
    "with open('dataset/chatbot_eksyar.json','r') as f:\n",
    "    data = json.loads(f.read())\n",
    "    \n",
    "# Normalizing data\n",
    "df = pd.json_normalize(data, record_path =['items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing question to X\n",
    "question = df.questions\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(question)\n",
    "sequences = tokenizer.texts_to_sequences(question)\n",
    "# Banyak kata yang telah di tokenizer\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memasukan hasil tokenizer ke dalam X \n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCES_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Y\n",
    "# encode label because label alfabetic\n",
    "# menjadikan label ke kategorikal\n",
    "le = LabelEncoder()\n",
    "label = df.labels\n",
    "labelEncode = le.fit_transform(label)\n",
    "y = to_categorical(labelEncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagi data menjadi 2 train dan test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=20, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = 'data-target-model-wtl.h5'\n",
    "# early_stopper = EarlyStopping(monitor='val_accuracy', patience=5, verbose=0, mode='auto')\n",
    "# checkpointer = ModelCheckpoint(filepath=checkpoint_path, verbose=0, save_best_only=True)\n",
    "\n",
    "# tinggal memnambahkan callback pada model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model CNN\n",
    "def build_cnn_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCES_LENGTH))\n",
    "\n",
    "    model.add(Conv1D(64, 3, padding='same', activation='relu'))\n",
    "    # model.add(MaxPooling1D())\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(units=250,activation=\"relu\"))  \n",
    "    model.add(Dense(units=5,activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer=\"adam\",metrics=[\"accuracy\"],loss=\"categorical_crossentropy\")\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "cnn_model = build_cnn_model()\n",
    "cnn_history = cnn_model.fit(X_train,y_train,epochs=30,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "ypred = cnn_model.predict(X_test)\n",
    "cnn_accuracy = accuracy_score(y_test.argmax(axis=-1),ypred.argmax(axis=-1))\n",
    "#print(\"CNN Accuracy:\",cnn_accuracy)\n",
    "cnn_cn = confusion_matrix(y_test.argmax(axis=-1),ypred.argmax(axis=-1))\n",
    "plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cnn_cn,annot=True,fmt=\"1d\",cbar=False,xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "plt.title(\"CNN Accuracy: {}\".format(cnn_accuracy),fontsize=50)\n",
    "plt.xlabel(\"Predicted\",fontsize=15)\n",
    "plt.ylabel(\"Actual\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, axe1 = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "axe1[0].plot(cnn_history.history[\"accuracy\"],label=\"accuracy\",color=\"blue\")\n",
    "axe1[1].plot(cnn_history.history[\"loss\"],label=\"loss\",color=\"red\")\n",
    "axe1[0].title.set_text(\"CNN Accuracy\")\n",
    "axe1[1].title.set_text(\"CNN Loss\")\n",
    "axe1[0].set_xlabel(\"Epoch\")\n",
    "axe1[1].set_xlabel(\"Epoch\")\n",
    "axe1[0].set_ylabel(\"Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_predict(quest):\n",
    "    puretext = tokenizer.texts_to_sequences(quest)\n",
    "    text_pad = pad_sequences(puretext,maxlen=50,padding='post')\n",
    "    predicted = cnn_model.predict(text_pad)\n",
    "    predicted_category = predicted.argmax(axis=1)\n",
    "    return le.classes_[predicted_category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quest = ['apa prinsip-prinsip investasi berdasarkan syariah']\n",
    "print(\"CNN Predicted Category: {}\".format(cnn_predict(quest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing code untuk menghasilkan nilai y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quest = ['apa prinsip ekonomi syariah?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puretext = tokenizer.texts_to_sequences(quest)\n",
    "text_pad = pad_sequences(puretext,maxlen=50,padding='post')\n",
    "predicted = cnn_model.predict(text_pad)\n",
    "predicted_category = predicted.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puretext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_[predicted_category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menngunakan Rake untuk mencari jawaban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algoritma MIT Licesence https://github.com/csurfer/rake-nltk\n",
    "from rake_nltk import Rake, Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO CONTROL STOPWORD\n",
    "\n",
    "    r = Rake(\n",
    "        stopwords=<list of stopwords>,\n",
    "        punctuations=<string of puntuations to ignore>\n",
    "    )\n",
    "    \n",
    "TO CONTROL METRIC FOR RANKING\n",
    "\n",
    "    # Paper uses d(w)/f(w) as the final metric. You can use this API with the\n",
    "    # following metrics:\n",
    "\n",
    "    # 1. d(w)/f(w) (Default metric) Ratio of degree of word to its frequency.\n",
    "    r = Rake(ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO)\n",
    "\n",
    "    # 2. d(w) Degree of word only.\n",
    "    r = Rake(ranking_metric=Metric.WORD_DEGREE)\n",
    "\n",
    "    # 3. f(w) Frequency of word only.\n",
    "    r = Rake(ranking_metric=Metric.WORD_FREQUENCY)\n",
    " \n",
    "TO CONTROL THE MAX OR MIN WORDS IN A PHARSE\n",
    "\n",
    "    r = Rake(min_length=2, max_length=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi stopword indonesia\n",
    "r = Rake(language=\"indonesian\", min_length=1, max_length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textsu = \"muhammad rizki aditia kenapa bisa adalah adanya adapun ataukah ataupun awal awalnya bagai bagaikan\"\n",
    "r.extract_keywords_from_text(textsu)\n",
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.extract_keywords_from_sentences(question)\n",
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "quests = \"Bagaimana bank konvensional menurut Islam?\"\n",
    "r.extract_keywords_from_text(quests)\n",
    "sentences = r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.0, 'bank konvensional'), (1.0, 'islam')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menghitung hasil dari Accuracy dan Precision dari Model CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cnn_model.predict(X_test)\n",
    "predictions = [np.argmax(predictions[i]) for i in range(len(predictions))]\n",
    "predictions = np.array(predictions)\n",
    "labels = [np.argmax(y_test[i]) for i in range(len(y_test))]\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print (\"Accuracy: \" + str(100*metrics.accuracy_score(labels, predictions)))\n",
    "print (\"Precision: \" + str(100*metrics.precision_score(labels, predictions, average=\"weighted\")))\n",
    "print (\"Recall: \" + str(100*metrics.recall_score(labels, predictions, average=\"weighted\")))\n",
    "print (\"f1_score: \" + str(100*metrics.f1_score(labels, predictions, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
