{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034c4e9-7e31-4dd8-bf57-62a11e692b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library yang dibutuhkan\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import keras\n",
    "import keras.utils\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import time\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4763cff-e91b-4caa-943e-ba3034600f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc4be7-67e3-4506-bb4c-9bb393270ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting beberapa variable yang akan digunakan\n",
    "MAX_SEQUENCES_LENGTH = 50 # Maximum kata pada kalimat berjumlah 50 \n",
    "MAX_NB_WORDS = 1000 # Maximum Vocabulary size\n",
    "EMBEDDING_DIM = 50 # Dimensions of Glove word vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b2fcb-7cdb-4cd2-b194-5ff8e35708a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data using Python JSON module\n",
    "import json\n",
    "with open('dataset/dataset.json','r') as f:\n",
    "    data = json.loads(f.read())\n",
    "    \n",
    "# Normalizing data\n",
    "df = pd.json_normalize(data, record_path =['items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b88396a-8a47-4d94-8243-bf4c16d71560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5caac9-0a14-4520-941b-884c8cd2d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan Preprocesing Data Question\n",
    "# Regex (Menghapus angka dan tanda baca) \n",
    "# Case Folding (merubah menjadi lower case)\n",
    "\n",
    "punctuation = '!\"#$%&()*+,./:;<=>?@[\\]^_`{|}~\\'0123456789'\n",
    "# defining the function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "# Storing the puntuation free text\n",
    "# Menjadikan df['punc'] sebagai Lower Case\n",
    "df['punc']= df['questions'].apply(lambda x:remove_punctuation(x).lower())\n",
    "df['punc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd1966-d61a-4c6b-9a03-5e73ec8003c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZATION\n",
    "# Untuk menormalisasi dan menyamakan beberapa kata\n",
    "with open('normalisasi.json','r') as f:\n",
    "    data = json.loads(f.read())\n",
    "    \n",
    "# Normalizing data\n",
    "normalizad_word = pd.json_normalize(data, record_path =['items'])\n",
    "normalizad_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b983c8-68cd-4ec8-996f-bd84407aedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenizing\n",
    "# 2. Normalisasi\n",
    "# 3. DeTokenize (dikarenakan akan dilakukan menggunakan library keras)\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "# Fungsi untuk melakukan Tokenizing\n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Fungsi untuk melakukan normalisasi\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "# Fungsi untuk melakukan detokenized\n",
    "def detoken(text):\n",
    "    text_detokenize = TreebankWordDetokenizer().detokenize(text)\n",
    "    return text_detokenize\n",
    "\n",
    "# Memasukan hasil dari penghapusan kalimat ke dalam tokenize\n",
    "df['tokenize'] = df['punc'].apply(word_tokenize_wrapper)\n",
    "\n",
    "normalizad_word_dict = {}\n",
    "\n",
    "for index, row in normalizad_word.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1] \n",
    "        \n",
    "# Merubah Kata menjadi normalisasi\n",
    "df['normalized'] = df['tokenize'].apply(normalized_term)\n",
    "\n",
    "# Mengembalikan text normalisasi kembali ke teks\n",
    "# Karena agar memudah dalam melakukan tokenizing dengan melakukan library keras\n",
    "df['normalized'] = df['normalized'].apply(lambda x: detoken(x))\n",
    "\n",
    "df['normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac931e06-6d1a-4668-99b7-63ecdd2362bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stemming\n",
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# factory = StemmerFactory()\n",
    "# stemmer = factory.create_stemmer()\n",
    "\n",
    "# def stemming(text):\n",
    "#     stream = stemmer.stem(text)\n",
    "#     return stream\n",
    "\n",
    "# df['stemming'] = df['normalized'].apply(lambda x: stemming(x))\n",
    "# df['stemming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd519a9-3b7f-43ee-bcb9-59254b929e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab844182-82cc-4de8-b97e-88465dcfb93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to json\n",
    "new_data = [df['labels'], df['questions'], df['answers'], df['normalized'], df['stemming']]\n",
    "headers = ['labels', 'questions', 'answers', 'normalized', 'stemming']\n",
    "new_df = pd.concat(new_data, axis=1, keys=headers)\n",
    "\n",
    "\n",
    "new_df.to_json('server/v1.1.0/dataset/data_final.json', orient='records')\n",
    "# lakukan beautiful json di internet\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583ec1a-a481-4213-b9a7-0ad38e048824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing question to X\n",
    "questions = df.normalized\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(questions)\n",
    "sequences = tokenizer.texts_to_sequences(questions)\n",
    "# Banyak kata yang telah di tokenizer\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c3875-bd1c-42fa-86f6-513fa745bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad3a46-c7ae-427e-8704-91bea505498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memasukan hasil tokenizer ke dalam X \n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCES_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76616f-2216-4be9-be95-05a16d52c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Y\n",
    "# encode label because label alfabetic\n",
    "# menjadikan label ke kategorikal\n",
    "le = LabelEncoder()\n",
    "label = df.labels\n",
    "labelEncode = le.fit_transform(label)\n",
    "y = to_categorical(labelEncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0a732-7875-4bd1-88fc-45d6402d20ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show lebel yang telah diencode\n",
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8988c5-60b7-427b-85a3-37b48b0ef95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagi data menjadi 2 train dan test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0380ea3-6033-470a-a7a7-fd21cf0f3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hasil dari split dataset\n",
    "# import pickle\n",
    "# with open('pengujian/final/split_dataset/datset60.pickle', 'wb') as f:\n",
    "#     pickle.dump([X_train, X_test, y_train, y_test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0de17-9b71-48b0-bda3-7ce2aaff4101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load split dataset\n",
    "# with open('pengujian/final/split_dataset/datset60.pickle', 'rb') as f:\n",
    "#     X_train, X_test, y_train, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb74ead-05e2-486e-b030-e5a7c752e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing Glove\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('dataset/glove/vectors.txt'), encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41703171-2b90-43be-a854-73408a688be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1fc1f3-4982-413d-9f48-123611596d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model CNN\n",
    "def build_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCES_LENGTH,weights=[embedding_matrix],trainable=True))\n",
    "    model.add(Conv1D(128, 3, padding='same', activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(64,activation=\"relu\")) \n",
    "    model.add(Dense(15, activation='softmax'))\n",
    "    model.compile(optimizer=\"adam\",metrics=[\"accuracy\"],loss=\"categorical_crossentropy\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb5e64-0e81-464d-8adb-a93a19131389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model cnn\n",
    "# cnn_model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699c4aa-ee71-4cd0-bd35-e24bbc4a407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# from keras.models import load_model\n",
    "# cnn_model = load_model('pengujian/final/training60_e20/model60_e20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3727a9-6fff-4622-8fcb-93c2095c323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "cnn_model = build_cnn_model()\n",
    "cnn_history = cnn_model.fit(X_train,y_train,epochs=30,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cd7b0-7ff3-40df-b961-591e592cbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(cnn_model, to_file='model.png', show_shapes=True, show_dtype=False, show_layer_names=True, rankdir='TB', expand_nested=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525432c-d4d8-4d31-9162-1caf2fff5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "ypred = cnn_model.predict(X_test)\n",
    "cnn_accuracy = accuracy_score(y_test.argmax(axis=-1),ypred.argmax(axis=-1))\n",
    "#print(\"CNN Accuracy:\",cnn_accuracy)\n",
    "cnn_cn = confusion_matrix(y_test.argmax(axis=-1),ypred.argmax(axis=-1))\n",
    "plt.subplots(figsize=(18,14))\n",
    "sns.heatmap(cnn_cn,annot=True,fmt=\"1d\",cbar=False,xticklabels=le.classes_,yticklabels=le.classes_)\n",
    "plt.title(\"CNN Accuracy: {}\".format(cnn_accuracy),fontsize=50)\n",
    "plt.xlabel(\"Predicted\",fontsize=15)\n",
    "plt.ylabel(\"Actual\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e821509-786c-4950-9e89-674b3ae5afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, axe1 = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "axe1[0].plot(cnn_history.history[\"accuracy\"],label=\"accuracy\",color=\"blue\")\n",
    "axe1[1].plot(cnn_history.history[\"loss\"],label=\"loss\",color=\"red\")\n",
    "axe1[0].title.set_text(\"CNN Accuracy\")\n",
    "axe1[1].title.set_text(\"CNN Loss\")\n",
    "axe1[0].set_xlabel(\"Epoch\")\n",
    "axe1[1].set_xlabel(\"Epoch\")\n",
    "axe1[0].set_ylabel(\"Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf66b11-bb7f-466d-ad2a-08497369356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test.argmax(axis=-1), ypred.argmax(axis=-1),target_names= le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a40b8-8971-4a67-b74d-6a39586d13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cnn_model.predict(X_test)\n",
    "predictions = [np.argmax(predictions[i]) for i in range(len(predictions))]\n",
    "predictions = np.array(predictions)\n",
    "labels = [np.argmax(y_test[i]) for i in range(len(y_test80))]\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a97f5-6165-4613-9c68-99dddef4e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print (\"Accuracy: \" + str(100*metrics.accuracy_score(labels, predictions)))\n",
    "print (\"Precision: \" + str(100*metrics.precision_score(labels, predictions, average=\"weighted\")))\n",
    "print (\"Recall: \" + str(100*metrics.recall_score(labels, predictions, average=\"weighted\")))\n",
    "print (\"f1_score: \" + str(100*metrics.f1_score(labels, predictions, average=\"weighted\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
